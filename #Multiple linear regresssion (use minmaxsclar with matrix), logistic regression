import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 데이터 로드
with open('./iris.csv', 'r') as f:
    df = pd.read_csv(f)

# Versicolor & Virginica 데이터만 사용 (이진 분류)
df = df[df['variety'].isin(['Versicolor', 'Virginica'])]
df['variety'] = df['variety'].map({'Versicolor': 0, 'Virginica': 1})

# 특성(X1, X2) 및 타겟(y)
X1 = df['sepal.width'].values  # (N,)
X2 = df['petal.length'].values  # (N,)
y = df['variety'].values  # (N,)

# 행렬 형태로 변환
X = np.c_[np.ones_like(X1), X1, X2]  # (N,3) (바이어스 포함)
W = np.random.uniform(low=-1.0, high=1.0, size=3)  # (3,) 벡터


# 학습 설정
num_iter = 1000
learning_rate = 0.001

# 경사 하강법 학습
for iteration in range(num_iter):
    Z = np.dot(X, W)  # (N,3) @ (3,) = (N,)
    y_hat = 1 / (1 + np.exp(-Z))  # 시그모이드 활성화

    gradient = np.dot(X.T, (y_hat - y)) / len(y)  # (3,)
    W -= learning_rate * gradient  # 벡터 연산으로 업데이트

# 결정 경계 시각화
x_min, x_max = X1.min() - 0.1, X1.max() + 0.1
y_min, y_max = X2.min() - 0.1, X2.max() + 0.1
xx1, xx2 = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))  # (100,100)

Z_decision = W[0] + W[1] * xx1 + W[2] * xx2  # (z도 100,100 행렬)
y_pred_decision = 1 / (1 + np.exp(-Z_decision))

plt.contourf(xx1, xx2, y_pred_decision, levels=[0, 0.5, 1], alpha=0.3, cmap='RdYlBu')

# True Labels vs Predicted Labels
colors_true = np.where(y == 1, 'red', 'blue')
y_hat_final = 1 / (1 + np.exp(np.dot(X, W)))
colors_pred = np.where(y_hat_final > 0.5, 'red', 'blue')

plt.scatter(X1, X2, c=colors_true, s=100, label='True Labels')
plt.scatter(X1, X2, edgecolor=colors_pred, facecolor='none', s=100, linewidth=2, label='Predicted Labels')


# 그래프 설정
plt.xlabel('X1 (Sepal Width)')
plt.ylabel('X2 (Petal Length)')
plt.title('Decision Boundary with True and Predicted Labels')
plt.legend()
plt.show()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 데이터 불러오기
dataset = pd.read_csv("advertising.csv")

# 입력 변수 및 타겟 변수 설정
X1 = dataset.iloc[:, :1].values  #Flatten사용하지 않는 이유는 (,1)로 2D를 만들어 나중에 X0행렬을 만들 때 벡터를 층층이 쌓기 위함.
  
# 행렬에는 FLATTEN 사용시 EX) (N,D) -> (N*D,)로 변하는 문제 벡터에만 사용
#-----벡터 벡터 연산------
#                 .dot결과     @결과
#(3,) @ (3,1)	스칼라(0D)	스칼라(0D)	✅ 내적 연산 가능  @=.dot 동일 기작
#(3,1) @ (3,1)	오류	오류	🚨 차원 불일치 (연산 불가능) -> 이 경우는 .T로 차원을 맞춰야하는 귀찮
#(1,3) @ (3,1)	(1,1) 행렬	(1,1) 행렬	✅ 행렬 연산 가능
#(3,1) @ (3,)	🚨 오류	🚨 오류	❌ 행렬 곱 조건 불만족 (a.T 필요)  
#(3,)벡터형태가 먼저 오면 벡터연산으로 인식하나 (3,1)형태로 먼저 오면 행렬연산으로 인식하여 차원을 맞춰야함

#---- 행렬 벡터 OR 행렬 행렬 연산 -----

#(100,3) @ (3,)	✅ 가능	(100,) 벡터	행렬-벡터 곱 가능  
#(3,) @ (100,3)	❌ 오류	-	행렬 곱 조건 불만족 (3 ≠ 100)  , 행렬 벡터 연산은 벡터가 먼저 나오지 않음 Ax조건 만족해야함.
#(3,1).T @ (100,3).T	✅ 가능	(1,100) 행렬	형태 변환 후 행렬 곱 가능

X2 = dataset.iloc[:, 1:2].values
X3 = dataset.iloc[:, 2:3].values
y = dataset.iloc[:, -1].values

# 데이터 정규화
X1 = (X1 - np.min(X1)) / (np.max(X1) - np.min(X1))
X2 = (X2 - np.min(X2)) / (np.max(X2) - np.min(X2))
X3 = (X3 - np.min(X3)) / (np.max(X3) - np.min(X3))
y = (y - np.min(y)) / (np.max(y) - np.min(y))

# 데이터 행렬 생성
X0 = np.ones((len(X1), 1))  # X0 = 1 (바이어스 항) (200,1)
XM = np.c_[X0, X1, X2, X3]  # 입력 행렬 (200, 4) ,(200,1)벡터를 가로로 쌓은 벡터 즉 (데이터개수,W개수) ,(n,) (n,1)이든 상관없는데 한가지만 들어와야함
# c_는 2d로 만들어서 이어준다.

# 가중치 초기화
w1 = np.random.uniform(low=-1.0, high=1.0)
w2 = np.random.uniform(low=-1.0, high=1.0)
w3 = np.random.uniform(low=-1.0, high=1.0)
w0 = np.random.uniform(low=-1.0, high=1.0)
W = np.array([w0, w1, w2, w3])  # (4, )  numpy는 1D 형태임. 나중에 DOT,@에서 자동으로 차원을 맞추지 않고도(T 상관없이) 벡터,행렬 연산이 되도록 위함.

# 학습률 및 반복 횟수 설정
lr = 0.0001
num_iter = 5000

# 손실 저장 리스트
errors = []

# 경사 하강법 수행
for iteration in range(num_iter):

    # 예측값 계산
    y_pred = np.dot(XM, W) # (200,) 벡터형태
    
    # 오차 계산
    error = (((y_pred - y) ** 2) / 2).mean()   #y는 (200,1)이나 -,+,*,/ 계산 후 (200,)형태로 바뀜. 이를 차원 축소, 브로드캐스팅 효과
    errors.append(error)
    
# N = len(y)  # 샘플 수 (200)
#error = (np.dot((y_pred - y).T, (y_pred - y)) / (2 * N)).item() 이 경우도 가능 (XW-Y).T(XW-Y)/2N 의 LOSS FUNCTION

    
    # 조기 종료 조건
    if error < 0.0005:
        break

    # 기울기 계산 및 가중치 업데이트
    gradient = np.dot(XM.T, (y_pred - y)) / len(y)  #행렬 벡터 연산이니 직접 차원을 맞춰주기 위해 XM.T 취함.
    W -= lr * gradient

# 학습 곡선 출력
plt.figure(figsize=(10, 7))
plt.plot(errors)
plt.xlabel('Iteration')
plt.ylabel('Error')
plt.title('Error Convergence')
plt.show()
